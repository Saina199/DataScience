{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Students Grades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have proposed an interesting project that was inspired by the constant challenge for college students to how best allocate their time and effort, in accordance to their timetables, in order to achieve certain grades in their courses. \n",
    "\n",
    "I begin with the premise that most students do not settle for just barely passing their courses, but want to attain the highest possible grades.  If the available resources (e.g. time) were unlimited, the highest possible grades would obviously be A+ across the board.  However, given that resources are limited in reality (and not to mention the occasional parties), a student would likely be more interested in efficiency rather than straight As.  In other words, how does one acquire the highest grades (for some, maybe just a B average) with the least amount of effort?  \n",
    "\n",
    "I am quite sure students in colleges and universities are not equipped with data-driven information.  Every year, about 50 (and up to 100s) of students enroll in specific core courses and leave behind much more data than only the mean and standard deviation (normally the only 2 metrics recorded by academic staff).  This data-driven information can be very useful in helping students choose their courses efficiently.  Most often, however, students would decide on their timetable and plan their effort throughout the term by talking to their peers.  This type of information is more subjective and biased in comparison to what data can provide. \n",
    "\n",
    "I have managed to acquire anonymous student grades (with proper permission) for an undergraduate chemical engineering course from 2005 to 2013.  Based on this data, I came up with questions from the point of view of a student who is trying to plan his/her timetable and course load at the beginning of the school term. \n",
    "\n",
    "Before presenting my results, I would like to mention some observations which I made from analysing the raw data; they are quite interesting and non-obvious: \n",
    "\n",
    "-\tPerformance of the students are unusually good in two of the school years (2006 and 2013).  These two years were exactly followed by a year in which a higher percentage of students failed or almost failed.  The performance here is evaluated based on the difference between the numbers of students who did exceptionally well compared to the ones who did exceptionally poorly for every school year. \n",
    "\n",
    "Based on this observation, I strongly suspect that the professors adjusted the material’s (exams) degree of difficulty accordingly after receiving such performance.  This action could be even taken subconsciously. \n",
    "\n",
    "-\tThe other observation, which is not really unexpected, is the wide distribution of raw scores associated with a particular letter grade.  The distribution is even wider with more average grades -- like a B -- which is what the average student really strives for.  This suggests that the student’s final grade is not so much determined by his/her own performance, but where he/she stands relative to other students.  The question here is not whether the assignment of grades is fair or not; what is interesting is how a student can maximize his chances of a good grade beforehand when he does not have any idea of how other students will do in the class and how difficult the upcoming exams will be. \n",
    "\n",
    "\n",
    "All these observations and thoughts, along with my own personal concerns back in my student days, made me think of a model which is not based on subjectivity and which can provide the students with some real quantification towards their decisions and goals.  In other words, variables such as the size of the class, the difficulty of the exams, the strength of other students enrolled in the class, and the professor’s judgment on distributing the letter grades all play a role in the final grade of the student.  Therefore, I decided to define the question by only focusing on what the student can control, which is the amount of work/performance he is willing to put into that specific course.  The question would be “How much work should I put in to guarantee my grade B (or higher) in this course, considering the uncertainties that are created by the aforementioned variables?”  \n",
    "\n",
    "To answer this question, I considered the components of this specific course -- the assignments, midterm and final exam -- all in the unity scale regardless of the weight that the syllabus would assign to each.  One might argue that of course the final exam carries a higher weight than the assignments in most courses; that is true, but remember my main goal is to not confuse the student by throwing different numbers at him, but to provide him with one single number so that he can distribute his work accordingly.  My scheme is data-driven, so the importance of each different feature in the target value (the final letter grade) will be embedded into the model that will be trained on the data.  What I aim for is to let the student know that, for example, he would need to achieve 85% in all components of the course (assignments, midterms, and final exam), to guarantee an A in the given course.  Of course a student with lower marks might achieve that too, but recall the whole purpose of this project is to reduce subjectivity of the letter grade to different factors and narrow down the uncertainty associated with the other factors. \n",
    "\n",
    "To train the model, I have performed a few classification techniques on 75% of the data as the training set and tuned the parameters based on accuracy performance of the test data.  Five features are considered in this model training: assignments, midterm and final are the first three; and since both midterm and final are quite important, their multiplicative product is considered as the 4th feature.  The last feature is the probability of the students that are receiving every grade in every class.  To train the data, I found Naïve Bayesian, Random Forest, and Extra Tree classifications to be the most relevant amongst the classification techniques.  The performance of the latter two is significantly higher than Naïve Bayesian, and the Extra Tree classifier provides a slightly higher accuracy compared to Random Forest, which could become crucial when deciding on the exact percentage.  For example, the Random Forest classifier suggests 66% performance to achieve a B grade, while Extra-Tree classifies 66% as grade C.  Therefore, the model trained with Extra-Tree classifier should be considered the more conservative approach and is therefore the safest.  Also, note that the letter grades are clustered into 5 broad letter grades of A, B, C, D and F, as opposed to working with finer gradations (e.g. B- and B+).  Classification for higher number of groups is possible if we are provided with more data. \n",
    "\n",
    "I also had the opportunity to be provided with partial data on the latest summer term of this course, which is still ongoing.  This data includes the marks for half the assignments and the midterm exam.  To predict the final grade of this class, I was first required to predict the students’ final exam marks rather than assuming the same performance as for the midterm.  I first compared, based on past data, the distributions of the final exam to midterm for all 5 letter grades separately.  I realized that the first order statistics are very close for all grades, that is: all students tend to do better in the final exam of this course by at least 10 marks (0.1).  This is not surprising as students are usually more motivated after receiving their midterm marks.  However, still this improvement in marks is varying by 20 points around the mean, which is not insignificant.  This means that a student who received 60% in his midterm can still score 50%..  After this analysis, I used Gaussian distribution with the corresponding mean and standard deviation to simulate each student’s final exam mark.  The prediction of letter grade is applied using the previously trained model.  A bootstrapping on this data can also estimate the mean and standard deviation of this class’s grade points. \n",
    "\n",
    "In conclusion, I believe providing a student with this data-driven information at the beginning of the school term could positively influence the student’s distributed work load and lead to better final grades that are more representative of student’s highest potential.  Every university could deploy a similar scheme on the student data available for every course and provide the students enrolling in the specific courses with such quantifications.  In addition, this project could still be expanded with further personalization of the results.  One could provide such information to a student and, using additional information on the ability of the student, advise him further.  For example, based on previous performance of the student in other similar courses and understanding of his motivation and ability, one can guide him in making a more rational plan toward accomplishing the course.  The similarity between courses could also be quantified by a similarity matrix considering factors such as the importance of the course to a discipline, the size of the class, the seniority of the students taking that course, and so on. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#print pd.show_versions()\n",
    "dat1 = pd.read_excel('./grades/314 (2005).xls',skiprows=2)\n",
    "dat2 = pd.read_excel('./grades/314 (2006).xls',skiprows=2)\n",
    "dat3 = pd.read_excel('./grades/314 (2007).xls',skiprows=2)\n",
    "dat4 = pd.read_excel('./grades/314 (2008).xls',skiprows=2)\n",
    "dat5 = pd.read_excel('./grades/314 (2009).xls',skiprows=1)\n",
    "dat6 = pd.read_excel('./grades/314 (2010).xls',skiprows=1)\n",
    "dat7 = pd.read_excel('./grades/314 (2011).xls',skiprows=1)\n",
    "dat8 = pd.read_excel('./grades/314 (2012).xls',skiprows=1)\n",
    "dat9 = pd.read_excel('./grades/314 (2013).xlsx',skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class_Size 2005 = 75\n",
      "Class_Size 2006 = 71\n",
      "Class_Size 2007 = 93\n",
      "Class_Size 2008 = 88\n",
      "Class_Size 2009 = 84\n",
      "Class_Size 2010 = 83\n",
      "Class_Size 2011 = 99\n",
      "Class_Size 2012 = 85\n",
      "Class_Size 2013 = 116\n"
     ]
    }
   ],
   "source": [
    "print 'Class_Size 2005 = '+ str(len(dat1))\n",
    "print 'Class_Size 2006 = '+ str(len(dat2))\n",
    "print 'Class_Size 2007 = '+ str(len(dat3))\n",
    "print 'Class_Size 2008 = '+ str(len(dat4))\n",
    "print 'Class_Size 2009 = '+ str(len(dat5))\n",
    "print 'Class_Size 2010 = '+ str(len(dat6))\n",
    "print 'Class_Size 2011 = '+ str(len(dat7))\n",
    "print 'Class_Size 2012 = '+ str(len(dat8))\n",
    "print 'Class_Size 2013 = '+ str(len(dat9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class_bottom 2005 = 7\n",
      "Class_bottom 2006 = 1\n",
      "Class_bottom 2007 = 6\n",
      "Class_bottom 2008 = 5\n",
      "Class_bottom 2009 = 3\n",
      "Class_bottom 2010 = 4\n",
      "Class_bottom 2011 = 5\n",
      "Class_bottom 2012 = 10\n",
      "Class_bottom 2013 = 6\n"
     ]
    }
   ],
   "source": [
    "### Assuming no student wanna get F and D \n",
    "### and good students wanna pass with A+ or A\n",
    "print 'Class_bottom 2005 = '+ str(sum((dat1.Grade=='F')|(dat1.Grade=='D-')|(dat1.Grade=='D')|(dat1.Grade=='D+')))\n",
    "print 'Class_bottom 2006 = '+ str(sum((dat2.Grade=='F')|(dat2.Grade=='D-')|(dat2.Grade=='D')|(dat2.Grade=='D+')))\n",
    "print 'Class_bottom 2007 = '+ str(sum((dat3.Grade=='F')|(dat3.Grade=='D-')|(dat3.Grade=='D')|(dat3.Grade=='D+')))\n",
    "print 'Class_bottom 2008 = '+ str(sum((dat4.Grade=='F')|(dat4.Grade=='D-')|(dat4.Grade=='D')|(dat4.Grade=='D+')))\n",
    "print 'Class_bottom 2009 = '+ str(sum((dat5.Grade=='F')|(dat5.Grade=='D-')|(dat5.Grade=='D')|(dat5.Grade=='D+')))\n",
    "print 'Class_bottom 2010 = '+ str(sum((dat6.Grade=='F')|(dat6.Grade=='D-')|(dat6.Grade=='D')|(dat6.Grade=='D+')))\n",
    "print 'Class_bottom 2011 = '+ str(sum((dat7.Grade=='F')|(dat7.Grade=='D-')|(dat7.Grade=='D')|(dat7.Grade=='D+')))\n",
    "print 'Class_bottom 2012 = '+ str(sum((dat8.Grade=='F')|(dat8.Grade=='D-')|(dat8.Grade=='D')|(dat8.Grade=='D+')))\n",
    "print 'Class_bottom 2013 = '+ str(sum((dat9.Grade=='F')|(dat9.Grade=='D-')|(dat9.Grade=='D')|(dat9.Grade=='D+')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class_top 2005 = 11\n",
      "Class_top 2006 = 14\n",
      "Class_top 2007 = 13\n",
      "Class_top 2008 = 13\n",
      "Class_top 2009 = 12\n",
      "Class_top 2010 = 15\n",
      "Class_top 2011 = 12\n",
      "Class_top 2012 = 13\n",
      "Class_top 2013 = 16\n"
     ]
    }
   ],
   "source": [
    "### it seems like after a class bad performance the next year class does very well... \n",
    "### Professor seems to makke easier exame the following year.. 2006 and 2013\n",
    "\n",
    "print 'Class_top 2005 = '+ str(sum((dat1.Grade=='A+')|(dat1.Grade=='A')))\n",
    "print 'Class_top 2006 = '+ str(sum((dat2.Grade=='A+')|(dat2.Grade=='A')))\n",
    "print 'Class_top 2007 = '+ str(sum((dat3.Grade=='A+')|(dat3.Grade=='A')))\n",
    "print 'Class_top 2008 = '+ str(sum((dat4.Grade=='A+')|(dat4.Grade=='A')))\n",
    "print 'Class_top 2009 = '+ str(sum((dat5.Grade=='A+')|(dat5.Grade=='A')))\n",
    "print 'Class_top 2010 = '+ str(sum((dat6.Grade=='A+')|(dat6.Grade=='A')))\n",
    "print 'Class_top 2011 = '+ str(sum((dat7.Grade=='A+')|(dat7.Grade=='A')))\n",
    "print 'Class_top 2012 = '+ str(sum((dat8.Grade=='A+')|(dat8.Grade=='A')))\n",
    "print 'Class_top 2013 = '+ str(sum((dat9.Grade=='A+')|(dat9.Grade=='A')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num = len(dat1)+len(dat2)+len(dat3)+len(dat4)+len(dat5)+len(dat6)+len(dat7)+len(dat8)+len(dat9)\n",
    "values = pd.DataFrame(np.nan, index=range(0,num), columns= ['Aa','Mt','Fe'])\n",
    "values = pd.concat([dat1.ix[:,'Aa':],dat2.ix[:,'Aa':],\n",
    "                    dat3.ix[:,'Aa':],dat4.ix[:,'Aa':],dat5.ix[:,'Aa':],\n",
    "                      dat6.ix[:,'Aa':],dat7.ix[:,'Aa':],dat8.ix[:,'Aa':],\n",
    "                    dat9.ix[:,'Aa':]])\n",
    "values = values.reset_index(drop=True)\n",
    "\n",
    "labels = pd.DataFrame(np.nan, index=range(0,len(values)),columns=['lables'])\n",
    "labels = pd.concat([dat1['Grade'],dat2['Grade'],dat3['Grade'],dat4['Grade'],dat5['Grade'],dat6['Grade'],\n",
    "                    dat7['Grade'],dat8['Grade'],dat9['Grade']])\n",
    "\n",
    "labels[(labels=='A+')|(labels=='A-')|(labels=='A')] = 'A'\n",
    "labels[(labels=='B+')|(labels=='B')] = 'B'\n",
    "labels[(labels=='C+')|(labels=='C')|(labels=='B-')] = 'C'\n",
    "labels[(labels=='D+')|(labels=='D')|(labels=='C-')] = 'D'\n",
    "labels[(labels=='F')|(labels=='D-')] = 'F'\n",
    "labels = labels.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Std of grades A difference: 0.177153425783\n",
      "Std of grades B difference: 0.234620142817\n",
      "Std of grades C difference: 0.217932251032\n",
      "Std of grades D difference: 0.196370082633\n",
      "Std of grades F difference: 0.207862798879\n",
      "Std of grades A difference: 0.21495914775\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeYAAAFVCAYAAAA6zUwUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAMQIwQwAQIwQzAAAxMj/B/Ue1ufPtUqXAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xa2cd4e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#for grade in labels:\n",
    "#plt.figure()\n",
    "#values['Mt'][(labels=='A')].hist(bins=30,alpha=0.5)\n",
    "#values['Fe'][(labels=='A')].hist(bins=30,alpha=0.5)\n",
    "\n",
    "plt.figure()\n",
    "(values['Fe'][(labels=='A')]-values['Mt'][(labels=='A')]).hist(alpha=0.5)\n",
    "(values['Fe'][(labels=='B')]-values['Mt'][(labels=='B')]).hist(alpha=0.5)\n",
    "(values['Fe'][(labels=='C')]-values['Mt'][(labels=='C')]).hist(alpha=0.5)\n",
    "(values['Fe'][(labels=='D')]-values['Mt'][(labels=='D')]).hist(alpha=0.5)\n",
    "\n",
    "print 'Std of grades A difference: '+str((values['Fe'][(labels=='A')]-values['Mt'][(labels=='A')]).std(axis=0))\n",
    "print 'Std of grades B difference: '+str((values['Fe'][(labels=='B')]-values['Mt'][(labels=='B')]).std(axis=0))\n",
    "print 'Std of grades C difference: '+str((values['Fe'][(labels=='C')]-values['Mt'][(labels=='C')]).std(axis=0))\n",
    "print 'Std of grades D difference: '+str((values['Fe'][(labels=='D')]-values['Mt'][(labels=='D')]).std(axis=0))\n",
    "print 'Std of grades F difference: '+str((values['Fe'][(labels=='F')]-values['Mt'][(labels=='F')]).std(axis=0))\n",
    "print 'Std of grades A difference: '+str((values['Fe']-values['Mt']).std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Point Average of 2005: 2.73333333333\n",
      "Point Average of 2006: 2.9014084507\n",
      "Point Average of 2007: 2.68817204301\n",
      "Point Average of 2008: 2.82954545455\n",
      "Point Average of 2009: 2.79761904762\n",
      "Point Average of 2010: 2.86746987952\n",
      "Point Average of 2011: 2.74747474747\n",
      "Point Average of 2012: 2.57647058824\n",
      "Point Average of 2013: 2.81034482759\n"
     ]
    }
   ],
   "source": [
    "GradePoint = pd.DataFrame(0.0, index=xrange(num), columns=['GradePoints'])\n",
    "GradePoint[(labels=='A')] = 4.0\n",
    "GradePoint[(labels=='B')] = 3.0\n",
    "GradePoint[(labels=='C')] = 2.0\n",
    "GradePoint[(labels=='D')] = 1.0\n",
    "GradePoint[(labels=='F')] = 0.0\n",
    "\n",
    "l2=len(dat1)+len(dat2); l3=l2+len(dat3); l4=l3+len(dat4); l5=l4+len(dat5); \n",
    "l6=l5+len(dat6); l7=l6+len(dat7); l8=l7+len(dat8); \n",
    "\n",
    "print 'Point Average of 2005: ' +str(sum(GradePoint['GradePoints'][:len(dat1)])/(len(dat1)*1.0))\n",
    "print 'Point Average of 2006: ' +str(sum(GradePoint['GradePoints'][len(dat1):l2])/(len(dat2)*1.0))\n",
    "print 'Point Average of 2007: ' +str(sum(GradePoint['GradePoints'][l2:l3])/(len(dat3)*1.0))\n",
    "print 'Point Average of 2008: ' +str(sum(GradePoint['GradePoints'][l3:l4])/(len(dat4)*1.0))\n",
    "print 'Point Average of 2009: ' +str(sum(GradePoint['GradePoints'][l4:l5])/(len(dat5)*1.0))\n",
    "print 'Point Average of 2010: ' +str(sum(GradePoint['GradePoints'][l5:l6])/(len(dat6)*1.0))\n",
    "print 'Point Average of 2011: ' +str(sum(GradePoint['GradePoints'][l6:l7])/(len(dat7)*1.0))\n",
    "print 'Point Average of 2012: ' +str(sum(GradePoint['GradePoints'][l7:l8])/(len(dat8)*1.0))\n",
    "print 'Point Average of 2013: ' +str(sum(GradePoint['GradePoints'][l8:])/(len(dat9)*1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59.9608032129 73.5016279495 51.0854070661\n",
      "59.315697601 62.6296768707 74.6585660339\n",
      "62.0024217329 66.5087165775 70.9498891626\n"
     ]
    }
   ],
   "source": [
    "Bstudent = pd.concat([dat1['FF(100)'],dat3['FF(100)'],dat4['FF(100)'],dat5['FF(100)'],\n",
    "                   dat6['FF(100)'],dat7['FF(100)'],dat8['FF(100)'],dat9['FF(100)']],axis=1)\n",
    "\n",
    "print np.mean(dat1['FF(100)']), np.mean(dat2['FF(100)']),np.mean(dat3['FF(100)'])\n",
    "print np.mean(dat4['FF(100)']), np.mean(dat5['FF(100)']),np.mean(dat6['FF(100)'])\n",
    "print np.mean(dat7['FF(100)']), np.mean(dat8['FF(100)']),np.mean(dat9['FF(100)'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
